{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LEQEe0luCvEj"
   },
   "source": [
    "# 全人類がわかるディープラーニング Day5演習\n",
    "\n",
    "## 概要\n",
    "\n",
    "本演習では自然言語処理に代表される系列データを扱うためのネットワークであるRNNやその派生ネットワークによる学習を穴埋め形式で実装します。なお、予め用意されたコードはそのまま使用し、指示された穴埋め部を編集してください。\n",
    "演習問題文は<font color=\"Red\">赤字</font>です。このファイルは必ず最後までコードをすべて実行し、「最後までコードが実行可能」・「学習結果の出力がある」・「学習が成功している」の３つを満たした状態で提出してください。\n",
    "\n",
    "所要時間：3~8時間"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sm7ZvF2SCvEk"
   },
   "source": [
    "### ライブラリのインポート\n",
    "\n",
    "必要なライブラリをインポートします。エラーになる場合は該当するものをインストールしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-thDJ_VCvEl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 乱数シードを指定\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2zc3aStCvEn"
   },
   "source": [
    "## 基本的関数、アルゴリズムの実装\n",
    "\n",
    "- SGD\n",
    "- Adam\n",
    "- sigmoid\n",
    "- softmax\n",
    "- clip_grads\n",
    "    勾配クリッピング用の関数。実装自体は単純なので省略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dM-1Xrc-Aye"
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(x.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RCuXbNbACvEo"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]\n",
    "            \n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-iaf2EbCvEp"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQF7Uc98CvFM"
   },
   "outputs": [],
   "source": [
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "icm4E44X-Ayo"
   },
   "source": [
    "## 自然言語処理用のレイヤークラス定義\n",
    "### Embedding クラス\n",
    "- 自然言語処理において、単語をベクトル（分散表現）に変換する前処理(Embeddingと言われる)が必須となる。\n",
    "- Embeddingクラスは入力されてきた単語列をべクトルに変換する。\n",
    "- 単語はID化されているため、一つのデータ（文章）は`[3,0,4,1]`のようになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sg0VFsMT-Ayp"
   },
   "source": [
    "例えば\n",
    "\n",
    "単語 0 を `[0.5, 1.2]`\n",
    "\n",
    "単語 1 を`[-1.4, 0.7]`\n",
    "\n",
    "単語 2 を` [2.5, -0.9]`\n",
    "\n",
    "単語 3 を`[5.6, 9.8]`\n",
    "\n",
    "単語 4 を`[-2.3, -0.8]`\n",
    "\n",
    "に変換するようなEmbed 層を用意する場合、\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "0.5 & 1.2 \\\\\n",
    "-1.4 & 0.7 \\\\\n",
    "2.5 & -0.9 \\\\\n",
    "5.6 & 9.8 \\\\\n",
    "-2.3 & -0.8 \\\\\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "のような変換行列を用意し、入力の単語IDをindexとして各単語を変換してやれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zemct8OW-Ayq"
   },
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBqVDDVq-Ayt"
   },
   "source": [
    "逆伝播については、変換行列に対して、入力時の単語に相当する箇所に伝播してきた勾配を足し合わせてやれば良いため、`np.add.at` を使用すれば良い。\n",
    "\n",
    "`np.add.at(dW, self.idx, dout)` によって、`dW[self.idx]`に対して`dout`が加えられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LdNoWAOa-Ayu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = np.array([[0.5, 1.2],\n",
    "              [-1.4, 0.7],\n",
    "              [2.5, -0.9],\n",
    "              [5.6, 9.8],\n",
    "              [-2.3, -0.8]])\n",
    "embedder_example = Embedding(w)\n",
    "minibatch_example = np.array([3,0,4,1])\n",
    "embedder_example.forward(minibatch_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1SI5FXlf-Ayw"
   },
   "source": [
    "## Time層の実装\n",
    "RNNで系列データを扱う際、例えば時刻tでの入力データ$x_t$を順伝播させたのちに隠れ層の値$h_t$を得、次の時刻t+1での入力データ$x_{t+1}$を順伝播させ……というように、データを時刻ごとに分割してからRNN層やAffine層、Embedding層に入力するという形式を取らなくてはならず、系列データを一々分割し、各時刻データに対して順伝播をさせるというように手間がかかってしまう。\n",
    "\n",
    "そのため、全時刻に渡って一度に順伝播などの処理を行ってくれるように、ユニットを時刻方向に繋げ一つのユニットとすることを考える。\n",
    "ここではこのように時系列データをまとめて扱う層のことを`TimeEmbedding`や`TimeAffine`のように、各クラス名に`Time`を付けて表すことにする。\n",
    "\n",
    "<img src= time_layer_image.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_FnjBns-Ayx"
   },
   "source": [
    "### TimeEmbedding クラス\n",
    "\n",
    "TimeEmbedding クラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）\n",
    "\n",
    "であり、各単語をD次元ベクトルに変換するとすれば出力は\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（次元数D）\n",
    "\n",
    "となる。\n",
    "\n",
    "系列に発展させた形としては、順伝播/逆伝播ともに時刻数tをfor文で順に処理してやれば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MO8XOzIWCvEr"
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0YWRoPBX-Ay1"
   },
   "source": [
    "### TimeAffine クラス\n",
    "TimeAffineクラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（単語ベクトル次元数）\n",
    "\n",
    "であり、このAffineクラスによって単語ベクトル次元数$D_1$から$D_2$に変換すると考えれば、サイズ$D_1 \\times D_2$ の重み行列を用意し、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（$D_2$）\n",
    "\n",
    "が出力となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8oo7Gr_CvEu"
   },
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hvSTZU-1-Ay5"
   },
   "source": [
    "### TimeSoftmaxWithLoss クラス\n",
    "TimeSoftmaxWithLoss クラスの入力として想定する形式は、\n",
    "\n",
    "（バッチサイズ）×（単語数=時刻数）×（次元数）\n",
    "\n",
    "であり、この次元数についてsoftmax関数により変換を行うものと考える。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3Sr6xoPCvEs"
   },
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        # バッチ分と時系列分をまとめる（reshape）\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= len(ts)\n",
    "\n",
    "        self.cache = (ts, ys, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= len(ys)\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIYsJAPZCvEy"
   },
   "source": [
    "## データセット用意\n",
    "\n",
    "ptbという英語の文章を集めたデータセットを用意します。\n",
    "\n",
    "このデータセットについて、単語を順に入力していき、次の単語を予測するというタスクを解かせてみます。\n",
    "本来であれば訓練データで学習をさせた後テストデータで検証を行いますが、簡単のため今回は学習のみを行い、テストによる検証は行いません。\n",
    "\n",
    "こちらのデータセットを用いて解いていくタスクは実践的なタスクと言うよりは、RNNの実装がうまくいっており、学習が成功することを確認するためのものとなります。\n",
    "\n",
    "データセットの用意の部分ですので、コードは読み飛ばしていただいても構いません。\n",
    "\n",
    "行っている処理の流れを示すと、\n",
    "1. データのダウンロード\n",
    "1. 単語とIDの変換ディクショナリの作成\n",
    "1. ダウンロードしてきた単語列のデータに対し、全単語をIDに変換\n",
    "\n",
    "と言う流れになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8psB7A9CvEz"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "import pickle\n",
    "\n",
    "url_base = 'https://raw.githubusercontent.com/tomsercu/lstm/master/data/'\n",
    "key_file = {\n",
    "    'train':'ptb.train.txt'\n",
    "}\n",
    "save_file = {\n",
    "    'train':'ptb.train.npy'\n",
    "}\n",
    "vocab_file = 'ptb.vocab.pkl'\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = './' + file_name\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print('Downloading ' + file_name + ' ... ')\n",
    "\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    except urllib.error.URLError:\n",
    "        import ssl\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "        urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "def load_vocab():\n",
    "    vocab_path = './' + vocab_file\n",
    "\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, 'rb') as f:\n",
    "            word_to_id, id_to_word = pickle.load(f)\n",
    "        return word_to_id, id_to_word\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    data_type = 'train'\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = './' + file_name\n",
    "\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word not in word_to_id:\n",
    "            tmp_id = len(word_to_id)\n",
    "            word_to_id[word] = tmp_id\n",
    "            id_to_word[tmp_id] = word\n",
    "\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump((word_to_id, id_to_word), f)\n",
    "\n",
    "    return word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9nT8sbgCvE1"
   },
   "outputs": [],
   "source": [
    "def load_ptb(data_type='train'):\n",
    "    save_path = './' + save_file[data_type]\n",
    "    word_to_id, id_to_word = load_vocab()\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        corpus = np.load(save_path)\n",
    "        return corpus, word_to_id, id_to_word\n",
    "\n",
    "    file_name = key_file[data_type]\n",
    "    file_path = './' + file_name\n",
    "    _download(file_name)\n",
    "\n",
    "    words = open(file_path).read().replace('\\n', '<eos>').strip().split()\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    np.save(save_path, corpus)\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbYK-qb0-AzF"
   },
   "source": [
    "読み込んだデータを学習用に分割します。\n",
    "そのままではデータ数が大きすぎるので、前から1000単語のみを使用します。\n",
    "\n",
    "xsには0番目から998番目までの単語IDが、tsには1番目から999番目までの単語IDが格納されています。解くべきタスクとしては、xsのi番目までの単語IDを入力として、i+1番目の単語、即ちtsのi番目の単語を予測するというものになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mM4f-cue-AzG"
   },
   "outputs": [],
   "source": [
    "# 学習データの読み込み（データセットを小さくする）\n",
    "corpus, word_to_id, id_to_word = load_ptb('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "\n",
    "xs = corpus[:-1]  # 入力\n",
    "ts = corpus[1:]  # 出力（教師ラベル）\n",
    "data_size = len(xs)\n",
    "print('corpus size: %d, vocabulary size: %d' % (corpus_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii6uCudrCvE4"
   },
   "source": [
    "## ネットワーク定義\n",
    "### RNNクラス\n",
    "問1-1. <font color=\"Red\">RNNレイヤーを表すクラス RNNクラスを完成させてください。</font>\n",
    "\n",
    "  RNNクラスは以下に従って定義します。\n",
    "    \n",
    "  - `Wx`, `Wh`はそれぞれ順伝播時に入力、前時刻隠れ層にかかる重み行列\n",
    "  - 活性化関数は `tanh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKhQecjFCvE4"
   },
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params \n",
    "        t =  ######問1.1.1######\n",
    "        h_next = np.tanh(t) \n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt =  ######問1.1.3######\n",
    "        db =  ######問1.1.4######\n",
    "        dWh =  ######問1.1.5######\n",
    "        dh_prev = ######問1.1.6######\n",
    "        dWx =  ######問1.1.7######\n",
    "        dx =  ######問1.1.8######\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qV66lPCN-AzM"
   },
   "source": [
    "### TimeRNNクラス\n",
    "問1-2. <font color=\"Red\">TimeRNNクラスを完成させてください。</font>\n",
    "\n",
    "TimeRNNクラスの仕様は以下の通りです。\n",
    "  \n",
    "  1. 順伝播\n",
    "      \n",
    "      - 時刻`t`を時系列分だけfor文でループさせます\n",
    "      - 順伝播により計算した隠れ層をメンバ変数`h`として格納します\n",
    "      - 順伝播時には<font color=\"Red\">メンバ変数`h`と入力データのうち時刻`t`に対応するデータを使用して</font>出力を計算します\n",
    "      \n",
    "  1. 逆伝播\n",
    "      - 今回のタスクにおいては、全時刻において次単語を予測し、その予測について損失が計算される、即ち勾配が入力されてくるため、逆伝播の入力`dhs`の想定される形は（バッチサイズ）×（単語数=時刻数）×（RNNの出力次元数）となります\n",
    "      - 各時刻において、<font color=\"Red\">次の時刻から伝播してきた勾配と現時刻における出力から伝播してきた勾配との和</font>を入力としてRNN層の逆伝播を計算します\n",
    "  \n",
    "また、今回のタスクにおける少々特殊な処理に対応するためや、この後実装する別タスクにおいても汎用的に使用するためにいくつか細かい仕様を加えます。\n",
    "実装上の細かい仕様のため、問題にはしていませんが、参考にしてください。\n",
    "\n",
    "  1. stateful\n",
    "    - 今回のタスクにおける目的はRNNネットワークが正しく実装できていることです。簡単に学習が成功していることを確認するため、（実務的にも用いられることのある）Truncated BPTT という少々特殊な処理を行います。\n",
    "    - 今回のタスクでは、入力する単語数（時刻数）として1000程度を入力していますが、この単語全てを順伝播させて逆伝播を計算し、ようやくパラメータ更新を1度行うというのでは効率が悪いため、順伝播は1000単語で行うものの逆伝播を5単語程度で区切って行う Truncated BPTT という方式を取っています。\n",
    "      1. まず、連続する5単語（例えば1~5番）を入力として順伝播・逆伝播・パラメータ更新を行います。\n",
    "      1. その際、順伝播の最終時刻における隠れ層`h`の値をメンバ変数として保持しておきます。\n",
    "      1. 次に5単語（先の例では6~10番）を入力として順伝播・逆伝播・パラメータ更新を行いますが、その際に初期入力の`h`を0で初期化せず、前から保持しているメンバ変数`h`の値をそのまま使用します。\n",
    "      1. このことにより、逆伝播は5単語単位で行われますが、順伝播は最初の単語から順にずっと計算されてきた値を使用することができます。\n",
    "      1. `stateful`がTrueの際にこのメンバ変数`h`の保持を行います。Falseの場合には`forward`関数が呼ばれるたびに`h`がリセット、即ち0で初期化されます。\n",
    "    \n",
    "  1. set_state\n",
    "    - 内部状態`h`を外から設定するための関数です。\n",
    "  \n",
    "  1. reset_state\n",
    "    - 内部状態`h`を外から削除するための関数です。\n",
    "  \n",
    "  1. params や grads としてパラメータ・勾配をまとめる\n",
    "    - 後でネットワーククラスを定義した際に、パラメータ更新などを楽に実行するための処理です。\n",
    "  1. backward における入力dhsの次元数による場合分け\n",
    "    - 今回のタスクでは全時刻において勾配が逆伝播してくるため、`dhs`の想定される形は（バッチサイズN）×（単語数=時刻数T）×（RNNの出力次元数H）となりますが、この後別のタスクを解く際には、全時刻ではなく最終時刻の出力のみから勾配が伝播してきます。\n",
    "    - この場合、dhsの次元数が違うため三次元に拡張することが必要になります。\n",
    "    - 最終時刻の出力のみから伝播してきたdhs の形は`(N,T,D)`のうち、`(N,D)`となっています。\n",
    "    - 最終時刻以外の勾配については何も伝播してきていないことから、全て0とすることで問題なく計算がなされます。\n",
    "    - 以上のことから、まず`np.zeros((N,T,D))`によって勾配の形を整え、その勾配の`[:,-1,:]`に対して入力された最終時刻における勾配を代入することで変わらず逆伝播を行うことができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHobocVhCvE6"
   },
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self,input_size, output_size, stateful=False):\n",
    "        D, H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        b = np.zeros(H).astype('f')\n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "        self.input_shapes = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        self.input_shapes = [N,T,D]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(######問1.2.1######) \n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = self.input_shapes\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        if dhs.ndim == 2:\n",
    "            temp = np.zeros((N,T,H))\n",
    "            temp[:,-1,:] = dhs\n",
    "            dhs = temp\n",
    "        \n",
    "        N, T, H = dhs.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(######問1.2.2######) \n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nDfIFVaw-AzQ"
   },
   "source": [
    "### SimpleRnnNetwork クラス\n",
    "バッチサイズN、単語数T、単語の種類数V、embedにより単語をベクトル化した際の次元数D、RNNによる出力次元数Hとします。\n",
    "\n",
    "入力されてくるデータのサイズは`(N,T)`であり\n",
    "\n",
    "TimeEmbedding層により`(N,T,D)`に変換\n",
    "\n",
    "TimeRNN層により`(N,T,H)`に変換\n",
    "\n",
    "TimeAffine層により`(N,T,V)`に変換した後、V次元の中で最大のものを予測単語IDとして出力します。\n",
    "損失については、Softmaxを使用してV種類の各単語について確率値を出力し、クロスエントロピーで計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIN7sid8CvE8"
   },
   "outputs": [],
   "source": [
    "class SimpleRnnNetwork:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ET7tV8h1-AzU"
   },
   "source": [
    "### ハイパーパラメータや学習用データセットの用意\n",
    "\n",
    "バッチサイズ10のミニバッチを作成するため、offsets や time_idx などを使用します。\n",
    "細かい挙動まで追う必要はありませんが、Truncated BPTT を行うため、バッチサイズ10, 単語数5のミニバッチを作成したあと、通常のようにまたランダムに連続した5単語を取るのではなく、先のミニバッチから連続した5単語をミニバッチとして選択する必要があります。\n",
    "\n",
    "例として簡単のために単語数100個、バッチサイズ2とした場合を考えます。\n",
    "\n",
    "1エポック目でのミニバッチ選択を追うと、\n",
    "\n",
    "```\n",
    "0番目の単語-1番目の単語-2番目の単語-3番目の単語-4番目の単語\n",
    "49番目の単語-50番目の単語-51番目の単語-52番目の単語-53番目の単語\n",
    "```\n",
    "    ↓\n",
    "```\n",
    "5番目の単語-6番目の単語-7番目の単語-8番目の単語-9番目の単語\n",
    "54番目の単語-55番目の単語-56番目の単語-57番目の単語-58番目の単語\n",
    "```\n",
    "    ↓\n",
    "    ……\n",
    "    \n",
    "と言うようにミニバッチ内の各データについて連続して単語を選択しています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_OX0jXUwCvE9"
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 20 #10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100\n",
    "time_size = 5 \n",
    "lr = 10 #0.1\n",
    "max_epoch = 40\n",
    "max_grad = 0.25\n",
    "\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "\n",
    "optimizer = SGD(lr)\n",
    "\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Xt6DtJDCvFB"
   },
   "source": [
    "## 学習、評価\n",
    "perplexity で評価を行います。\n",
    "\n",
    "40エポックでperplexityが一桁程度まで低下していれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7cDXCBHZ-Azb"
   },
   "outputs": [],
   "source": [
    "model = SimpleRnnNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "#model = LSTMNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "rnn_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    rnn_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyzEKNaLCvFD"
   },
   "source": [
    "## LSTM\n",
    "### LSTMクラス\n",
    "\n",
    "問2-1. <font color=\"Red\">LSTMクラスを完成させてください。</font>\n",
    "\n",
    "LSTMクラスの仕様はRNNクラスとほとんど同じです。\n",
    "\n",
    "各ゲートの計算と通常の順伝播の計算に使用するパラメータを行列にまとめ、一行で計算できるように実装しています。\n",
    "\n",
    "f,g,i,o はそれぞれ、忘却ゲート・入力からの順伝播・入力ゲート・出力ゲートを表しています。\n",
    "\n",
    "<img src=\"lstm_image.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "972frd2QCvFE"
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = ######問2.1.1######\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f =  ######問2.1.2######\n",
    "        g =  ######問2.1.3######\n",
    "        i =  ######問2.1.4######\n",
    "        o =  ######問2.1.5######\n",
    "\n",
    "        c_next =  ######問2.1.6######\n",
    "        h_next =  ######問2.1.7######\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds =  ######問2.1.8######\n",
    "\n",
    "        dc_prev =  ######問2.1.9######\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fSpv_YIw-Azh"
   },
   "source": [
    "### TimeLSTMクラス\n",
    "\n",
    "問2-2. <font color=\"Red\">TimeLSTMクラスを完成させてください。</font>\n",
    "\n",
    "TimeRNNと同じように実装します。\n",
    "\n",
    "SimpleRNN のときと違って、重みの初期化に注意を払う必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXkWl14bCvFF"
   },
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, input_size, output_size, stateful=False):\n",
    "        D,H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        Wx = (rn(######問2.2.1######) / np.sqrt(D)).astype('f') \n",
    "        Wh = (rn(######問2.2.2######) / np.sqrt(H)).astype('f') \n",
    "        b = np.zeros(######問2.2.3######).astype('f') \n",
    "\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "        self.input_shapes = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "        self.input_shapes = [N,T,D]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(######問2.2.4######) \n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        H = Wh.shape[0]\n",
    "        N, T, D = self.input_shapes\n",
    "        \n",
    "        if dhs.ndim == 2:\n",
    "            temp = np.zeros((N,T,H))\n",
    "            temp[:,-1,:] = dhs\n",
    "            dhs = temp\n",
    "  \n",
    "        N, T, H = dhs.shape\n",
    " \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(######問2.2.5######) \n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "STsi7N7c-Azk"
   },
   "source": [
    "### LSTMNetworkクラス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UIwu6fuCvFI"
   },
   "outputs": [],
   "source": [
    "class LSTMNetwork:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ofZZ-gIpCvFO"
   },
   "source": [
    "## 学習、評価\n",
    "\n",
    "ハイパーパラメータなどは先ほどのRNNと全て共通で学習させます。\n",
    "\n",
    "40エポックでperplexity が5以下となっていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcgBW6tmCvFP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTMNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "lstm_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    lstm_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fnkwvXuS-Azs"
   },
   "source": [
    "LSTMとRNNの学習結果を比較し、LSTMの方が安定して素早く学習が収束していることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBRkO629-Azt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(max_epoch), rnn_ppl_list)\n",
    "plt.plot(range(max_epoch), lstm_ppl_list)\n",
    "plt.legend([\"RNN\", \"LSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5a9St6Om-Azx"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4T4bucL-Azy"
   },
   "outputs": [],
   "source": [
    "if rnn_ppl_list[-1] > 10:\n",
    "    print(\"RNNの実装に間違いがあります。問1を見直してください。\")\n",
    "elif lstm_ppl_list[-1] > 5:\n",
    "    print(\"LSTMの実装に間違いがあります。問2を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2FutIo3-Az0"
   },
   "source": [
    "## GRU\n",
    "### GRUクラス\n",
    "\n",
    "問3-1. <font color=\"Red\">GRUクラスを完成させてください。</font>\n",
    "\n",
    "GRUクラスの仕様はLSTMクラスとほとんど同じです。\n",
    "\n",
    "各ゲートの計算と通常の順伝播の計算に使用するパラメータを行列にまとめ、一行で計算できるように実装しています。\n",
    "\n",
    "r がリセットゲート、zが更新ゲートを表します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3lrNddh-Az1"
   },
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(self, Wx, Wh):\n",
    "        self.params = [Wx, Wh]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh = self.params\n",
    "        H, H3 = Wh.shape\n",
    "        Wxz, Wxr, Wx = Wx[:, :H], Wx[:, H:2 * H], Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = Wh[:, :H], Wh[:, H:2 * H], Wh[:, 2 * H:]\n",
    "\n",
    "        z = sigmoid(np.dot(x, Wxz) + np.dot(h_prev, Whz))\n",
    "        r = sigmoid(np.dot(x, Wxr) + np.dot(h_prev, Whr))\n",
    "        h_hat = np.tanh(np.dot(x, Wx) + np.dot(r*h_prev, Wh))\n",
    "        h_next = ######問3.1.1######\n",
    "\n",
    "        self.cache = (x, h_prev, z, r, h_hat)\n",
    "\n",
    "        return h_next\n",
    "\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh = self.params\n",
    "        H, H3 = Wh.shape\n",
    "        Wxz, Wxr, Wx = Wx[:, :H], Wx[:, H:2 * H], Wx[:, 2 * H:]\n",
    "        Whz, Whr, Wh = Wh[:, :H], Wh[:, H:2 * H], Wh[:, 2 * H:]\n",
    "        x, h_prev, z, r, h_hat = self.cache\n",
    "\n",
    "        dh_hat =dh_next * z\n",
    "        dh_prev = dh_next * (1-z)\n",
    "\n",
    "        dt = dh_hat * (1 - h_hat ** 2)\n",
    "        dWh = np.dot((r * h_prev).T, dt)\n",
    "        dhr = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        dh_prev += r * dhr\n",
    "\n",
    "        dz = dh_next * h_hat - dh_next * h_prev\n",
    "        dt = dz * z * (1-z)\n",
    "        dWhz = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whz.T)\n",
    "        dWxz = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxz.T)\n",
    "\n",
    "        dr = dhr * h_prev\n",
    "        dt = dr * r * (1-r)\n",
    "        dWhr = np.dot(h_prev.T, dt)\n",
    "        dh_prev += np.dot(dt, Whr.T)\n",
    "        dWxr = np.dot(x.T, dt)\n",
    "        dx += np.dot(dt, Wxr.T)\n",
    "\n",
    "        dWx = np.hstack((dWxz, dWxr, dWx))\n",
    "        dWh = np.hstack((dWhz, dWhr, dWh))\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "\n",
    "\n",
    "        return dx, dh_prev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t7TYSfoi-Az2"
   },
   "source": [
    "### TimeGRUクラス\n",
    "\n",
    "問3-2. <font color=\"Red\">TimeGRUクラスを完成させてください。</font>\n",
    "\n",
    "TimeGRUクラスの仕様はTimeLSTMクラスとほとんど同じです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-jclSpgq-Az3"
   },
   "outputs": [],
   "source": [
    "class TimeGRU:\n",
    "    def __init__(self, input_size, output_size, stateful=False):\n",
    "        D, H = input_size, output_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        Wx = (rn(######問3.2.1######) / np.sqrt(D)).astype('f') \n",
    "        Wh = (rn(######問3.2.2######) / np.sqrt(H)).astype('f') \n",
    "\n",
    "        self.params = [Wx, Wh]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh)]\n",
    "        self.layers = None\n",
    "        self.h = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H, H3 = Wh.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = GRU(*self.params)\n",
    "            self.h = layer.forward(######問3.2.3######) \n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        grads = [0,0]\n",
    "        dh = 0\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(######問3.2.4######) \n",
    "\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "            \n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UxL7g9mu-Az5"
   },
   "source": [
    "### GRUNetworkクラス\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vrYrBg--Az6"
   },
   "outputs": [],
   "source": [
    "class GRUNetwork:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeGRU(D, H, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxbAcqGa-Az8"
   },
   "source": [
    "## 学習、評価\n",
    "\n",
    "ハイパーパラメータなどは先ほどのRNN・LSTMと全て共通で学習させます。\n",
    "\n",
    "40エポックでperplexity が5以下となっていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QL1EbV7v-Az9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GRUNetwork(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "gru_ppl_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for iter in range(max_iters):\n",
    "        # ミニバッチの取得\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "    # エポックごとにパープレキシティの評価\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print('| epoch %d | perplexity %.2f'\n",
    "          % (epoch+1, ppl))\n",
    "    gru_ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G80Gpmza-A0A"
   },
   "source": [
    "GRUとLSTM、RNNの学習結果を比較し、GRUがLSTMと同様（若しくはそれ以上）の性能を持っていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nyRMCYbX-A0B",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(max_epoch), rnn_ppl_list)\n",
    "plt.plot(range(max_epoch), lstm_ppl_list)\n",
    "plt.plot(range(max_epoch), gru_ppl_list)\n",
    "plt.legend([\"RNN\", \"LSTM\", \"GRU\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"perplexity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-QQl5QZ-A0D"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpWhGBYN-A0D"
   },
   "outputs": [],
   "source": [
    "if gru_ppl_list[-1] > 5:\n",
    "    print(\"GRUの実装に間違いがあります。問3を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9ZeqQfS-A0F"
   },
   "source": [
    "## Classification\n",
    "では実際にRNNやLSTMを用いて実践的な問題を解いていきましょう。\n",
    "\n",
    "今回解いていくタスクは二値分類なので、まずは出力層の活性化と損失関数をつなげたSigmoidWithLossクラスを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vrQMLkm-A0F"
   },
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        x = x.flatten()\n",
    "        self.y = sigmoid(x)\n",
    "        self.loss = -np.mean(np.log(self.y + 1e-7) * t + np.log(1 - self.y + 1e-7) * (1 - t))\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t)/batch_size\n",
    "        dx = dx.reshape(-1,1)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0r8DMHJ-A0H"
   },
   "source": [
    "## データセットの用意\n",
    "\n",
    "今回使用するデータは \"movie review\" と呼ばれる映画の評論記事の分類問題です。\n",
    "評論記事に対して「1(positive)」「0(negative)」の二値がラベルとして付与されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hBxRZA55-A0H"
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYaC0Zar0VIG"
   },
   "outputs": [],
   "source": [
    "def load_movie_reviews():\n",
    "    from nltk.corpus import movie_reviews\n",
    "    try:\n",
    "        movie_reviews.categories()\n",
    "    except:\n",
    "        import nltk\n",
    "        nltk.download('movie_reviews')\n",
    "        from nltk.corpus import movie_reviews\n",
    "    raw_data = []\n",
    "\n",
    "    # NLTK's corpus is structured in an interesting way\n",
    "    # first iterate through the two categories (pos and neg)\n",
    "    for category in movie_reviews.categories():\n",
    "\n",
    "        if category == 'pos':\n",
    "            label = '1'\n",
    "        elif category == 'neg':\n",
    "            label = '0'\n",
    "\n",
    "        # each of these categories is just fileids, so grab those\n",
    "        for fileid in movie_reviews.fileids(category):\n",
    "            # then each review is a NLTK class where each item in that class instance is a word\n",
    "            review_words = list(movie_reviews.words(fileid))\n",
    "            if len(review_words) >= 400:\n",
    "                review_words = review_words[:400]\n",
    "            else:\n",
    "                review_words.extend([\" \" for i in range(400 - len(review_words))])\n",
    "            review_dictionary = {\n",
    "                'text': review_words,\n",
    "                'label': label\n",
    "            }\n",
    "\n",
    "            raw_data.append(review_dictionary)\n",
    "\n",
    "    return raw_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3Q0F0VK-A0M"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"data.pkl\"):\n",
    "    f = open(\"data.pkl\", \"rb\")\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "else:\n",
    "    data = load_movie_reviews()\n",
    "    f = open(\"data.pkl\", \"wb\")\n",
    "    pickle.dump(data,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WUPxiQj4-A0O"
   },
   "source": [
    "## Embedder の用意\n",
    "\n",
    "先ほどの例ではEmbed層をネットワーク内に含めていましたが、Embed 層も含めて学習をすると時間がかかってしまうため、今回はあらかじめ用意された embedder を使用してネットワークに入れる前に単語列をベクトル化しておきます。\n",
    "\n",
    "embedderモデルのダウンロードには時間がかかりますのでご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z6IvEiQ5-A0R"
   },
   "outputs": [],
   "source": [
    "if os.path.exists(\"embedder.model\"):\n",
    "    model = word2vec.Word2VecKeyedVectors.load_word2vec_format(\"embedder.model\")\n",
    "else:\n",
    "    model = api.load(\"glove-twitter-25\")\n",
    "    model.save_word2vec_format(\"embedder.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oocxBSPH-A0T"
   },
   "outputs": [],
   "source": [
    "def embed_one_word_via_model(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vy8BecrW-A0V",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = []\n",
    "labels = []\n",
    "for d in data:\n",
    "    embedding.append(np.array([embed_one_word_via_model(word,model) for word in d[\"text\"]]))\n",
    "    labels.append(int(d[\"label\"]))\n",
    "embedding = np.array(embedding)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, T_train, T_test = train_test_split(embedding, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XtARJu9j-A0X"
   },
   "source": [
    "## ネットワーク の用意\n",
    "問4-1. <font color=\"Red\">以下の SimpleRNNClassifier, LSTMClassifier クラスを完成させてください。</font>\n",
    "\n",
    "先ほどの問題では各時刻の入力に対して一つずつ出力が計算されましたが、今回のタスクにおいては、全時刻の入力データに対して一つの出力を計算します。\n",
    "\n",
    "そのため、順伝播においてRNN層やLSTM層の出力に対して、時系列のうち最終出力のみを取り出し、Affine層に入力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WXX5-R79-A0Y"
   },
   "outputs": [],
   "source": [
    "class SimpleRNNClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.rnn_layer = TimeRNN(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.rnn_layer.params\n",
    "        self.grads += self.rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        xs = self.rnn_layer.forward(xs)[######問4.1.1######] \n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout = self.rnn_layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYFDYsE7-A0b"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        rnn_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(H, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.rnn_layer.params\n",
    "        self.grads += self.rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        xs = self.rnn_layer.forward(xs)[######問4.1.2######] \n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout = self.rnn_layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OXDRZyAX-A0e"
   },
   "source": [
    "## 学習の実行\n",
    "今回のタスクにおいては、RNNではうまく学習が行われず損失があまり減少してくれません。\n",
    "\n",
    "対してLSTMでは、環境によって結果に差は出ますが、RNNに比べると順調に損失が減少します。\n",
    "\n",
    "学習の実行には少々時間がかかるため、注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7GontpJ-A0f"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "eval_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl4z5nq7-A0h",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn_model = SimpleRNNClassifier(25, 100)\n",
    "lstm_model = LSTMClassifier(25, 100)\n",
    "optimizer1 = Adam(lr)\n",
    "optimizer2 = Adam(lr)\n",
    "batch_size = 100\n",
    "rnn_loss_list = []\n",
    "lstm_loss_list = []\n",
    "\n",
    "np.random.seed(0)\n",
    "for epoch in range(n_epoch):\n",
    "    total_rnn_loss = 0\n",
    "    total_lstm_loss = 0\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    for i, idx in enumerate(range(0, len(X_train), batch_size)):\n",
    "        X_batch = X_train[perm[idx:idx+batch_size]]\n",
    "        T_batch = T_train[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        rnn_loss = rnn_model.forward(X_batch, T_batch)\n",
    "        rnn_model.backward()\n",
    "        optimizer1.update(rnn_model.params, rnn_model.grads)\n",
    "        total_rnn_loss += rnn_loss*len(X_batch)\n",
    "        \n",
    "        lstm_loss = lstm_model.forward(X_batch, T_batch)\n",
    "        lstm_model.backward()\n",
    "        optimizer2.update(lstm_model.params, lstm_model.grads)\n",
    "        total_lstm_loss += lstm_loss*len(X_batch)\n",
    "        if i % eval_interval == 0:\n",
    "            print('| idx %d / %d | RNN loss %.2f | LSTM loss %.2f |'\n",
    "                 %(idx, len(X_train), total_rnn_loss/(idx+batch_size), total_lstm_loss/(idx+batch_size)))\n",
    "    average_rnn_loss = total_rnn_loss / len(X_train)\n",
    "    rnn_loss_list.append(average_rnn_loss)\n",
    "    average_lstm_loss = total_lstm_loss / len(X_train)\n",
    "    lstm_loss_list.append(average_lstm_loss)\n",
    "    rnn_pred = rnn_model.predict(X_test).flatten()\n",
    "    lstm_pred = lstm_model.predict(X_test).flatten()\n",
    "    rnn_accuracy = ((rnn_pred > 0) == T_test).mean() * 100\n",
    "    lstm_accuracy = ((lstm_pred > 0) == T_test).mean() * 100\n",
    "    print('| epoch %d | RNN loss %.2f | LSTM loss %.2f | RNN accuracy %.2f | LSTM accuracy %.2f'\n",
    "          % (epoch+1, average_rnn_loss, average_lstm_loss, rnn_accuracy, lstm_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HhOzPJ0-A0k"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(n_epoch), rnn_loss_list)\n",
    "plt.plot(range(n_epoch), lstm_loss_list)\n",
    "plt.legend([\"RNN\", \"LSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4m_igEHD-A0l"
   },
   "source": [
    "## bi-directional LSTM\n",
    "問4-2. <font color=\"Red\">以下の BidirectionalLSTMClassifier クラスを完成させてください。</font>\n",
    "\n",
    "入力系列データに対して、順方向のデータを処理するforward LSTM層と逆方向のデータを処理するbackward LSTM層を用意します。\n",
    "\n",
    "通常のLSTM層とパラメータ数が大きく変わらないように各LSTM層は hidden_size の半分の次元数を出力します。\n",
    "\n",
    "forward LSTM と backward LSTM の出力を横につなげ、affine層に入力し分類タスクを解きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgpTdphW-A0m"
   },
   "outputs": [],
   "source": [
    "class BidirectionalLSTMClassifier:\n",
    "    def __init__(self, wordvec_size, hidden_size):\n",
    "        D, H = wordvec_size, hidden_size //2\n",
    "        rn = np.random.randn\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # 重みの初期化\n",
    "        affine_W = (rn(H * 2, 1) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(1).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.affine_layer = Affine(affine_W, affine_b)\n",
    "        self.loss_layer = SigmoidWithLoss()\n",
    "        self.forward_rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "        self.backward_rnn_layer = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        self.params += self.forward_rnn_layer.params\n",
    "        self.grads += self.forward_rnn_layer.grads\n",
    "        self.params += self.backward_rnn_layer.params\n",
    "        self.grads += self.backward_rnn_layer.grads\n",
    "        self.params += self.affine_layer.params\n",
    "        self.grads += self.affine_layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        xs = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        forward_xs = self.forward_rnn_layer.forward(xs)[######問4.2.1######] \n",
    "        backward_xs = self.backward_rnn_layer.forward(xs[######問4.2.2######])[######問4.2.3######] \n",
    "        xs = np.hstack((forward_xs, backward_xs))\n",
    "        xs = self.affine_layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        dout = self.affine_layer.backward(dout)\n",
    "        dout1, dout2 = np.hsplit(dout, 2)\n",
    "        dout1 = self.forward_rnn_layer.backward(dout1)\n",
    "        dout2 = self.backward_rnn_layer.backward(dout2)\n",
    "        return (dout1, dout2)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.forward_rnn_layer.reset_state()\n",
    "        self.backward_rnn_layer.reset_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74sZCjOy-A0n"
   },
   "source": [
    "## 学習の実行\n",
    "Bidirectional LSTM がLSTMより良い性能を出していることを確認してください。\n",
    "\n",
    "10エポックで損失が0.2を下回っていれば学習成功です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1OwKNvA-A0o"
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "n_epoch = 10\n",
    "eval_interval = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNRv0_2K-A0p",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bilstm_model = BidirectionalLSTMClassifier(25, 100)\n",
    "optimizer = Adam(lr)\n",
    "batch_size = 100\n",
    "bilstm_loss_list = []\n",
    "np.random.seed(0)\n",
    "for epoch in range(n_epoch):\n",
    "    total_bilstm_loss = 0\n",
    "    perm = np.random.permutation(len(X_train))\n",
    "    for i, idx in enumerate(range(0, len(X_train), batch_size)):\n",
    "        X_batch = X_train[perm[idx:idx+batch_size]]\n",
    "        T_batch = T_train[perm[idx:idx+batch_size]]\n",
    "        \n",
    "        bilstm_loss = bilstm_model.forward(X_batch, T_batch)\n",
    "        bilstm_model.backward()\n",
    "        optimizer.update(bilstm_model.params, bilstm_model.grads)\n",
    "        total_bilstm_loss += bilstm_loss*len(X_batch)\n",
    "        if i % eval_interval == 0:\n",
    "            print('| idx %d / %d | BiLSTM loss %.2f |'\n",
    "                 %(idx, len(X_train), total_bilstm_loss/(idx+batch_size)))\n",
    "\n",
    "    average_bilstm_loss = total_bilstm_loss / len(X_train)\n",
    "    bilstm_loss_list.append(average_bilstm_loss)\n",
    "    bilstm_pred = bilstm_model.predict(X_test).flatten()\n",
    "    bilstm_accuracy = ((bilstm_pred > 0) == T_test).mean() * 100\n",
    "    print('| epoch %d | BiLSTM loss %.2f | BiLSTM accuracy %.2f'\n",
    "          % (epoch+1, average_bilstm_loss, bilstm_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNat1zIf-A0r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(n_epoch), rnn_loss_list)\n",
    "plt.plot(range(n_epoch), lstm_loss_list)\n",
    "plt.plot(range(n_epoch), bilstm_loss_list)\n",
    "plt.legend([\"RNN\", \"LSTM\", \"BiLSTM\"])\n",
    "plt.xlabel(\"#epoch\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NO6x3b-t-A0s"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ie0QOeJt-A0s"
   },
   "outputs": [],
   "source": [
    "if average_bilstm_loss > 0.2:\n",
    "    print(\"RNNの実装に間違いがあります。問4を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。次のステップに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IlswOIfWCvFR"
   },
   "source": [
    "## Seq2Seq\n",
    "## データセット用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOzLPGCk-A0u"
   },
   "source": [
    "Seq2seq を用いてタスクを解いていきます。\n",
    "\n",
    "今回用いるデータセットは、足し算の数式を並べたものになります。数式とその答を全て文字列として考え、足し算の式をLSTMによって一文字ずつ読み込み、その答えを一文字ずつ出力していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "000wqFK6CvFS"
   },
   "outputs": [],
   "source": [
    "id_to_char = {}\n",
    "char_to_id = {}\n",
    "\n",
    "\n",
    "def _update_vocab(txt):\n",
    "    chars = list(txt)\n",
    "\n",
    "    for i, char in enumerate(chars):\n",
    "        if char not in char_to_id:\n",
    "            tmp_id = len(char_to_id)\n",
    "            char_to_id[char] = tmp_id\n",
    "            id_to_char[tmp_id] = char\n",
    "\n",
    "def load_sequence(file_name='addition.txt'):\n",
    "    file_path = './' + file_name\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print('No file: %s' % file_name)\n",
    "        return None\n",
    "\n",
    "    questions, answers = [], []\n",
    "\n",
    "    for line in open(file_path, 'r'):\n",
    "        idx = line.find('_')\n",
    "        questions.append(line[:idx])\n",
    "        answers.append(line[idx:-1])\n",
    "\n",
    "    # create vocab dict\n",
    "    for i in range(len(questions)):\n",
    "        q, a = questions[i], answers[i]\n",
    "        _update_vocab(q)\n",
    "        _update_vocab(a)\n",
    "\n",
    "    # create np array\n",
    "    x = np.zeros((len(questions), len(questions[0])), dtype=np.int)\n",
    "    t = np.zeros((len(questions), len(answers[0])), dtype=np.int)\n",
    "\n",
    "    for i, sentence in enumerate(questions):\n",
    "        x[i] = [char_to_id[c] for c in list(sentence)]\n",
    "    for i, sentence in enumerate(answers):\n",
    "        t[i] = [char_to_id[c] for c in list(sentence)]\n",
    "\n",
    "    # shuffle\n",
    "    indices = np.arange(len(x))\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    t = t[indices]\n",
    "\n",
    "    # 10% for validation set\n",
    "    split_at = len(x) - len(x) // 10\n",
    "    (x_train, x_test) = x[:split_at], x[split_at:]\n",
    "    (t_train, t_test) = t[:split_at], t[split_at:]\n",
    "\n",
    "    return (x_train, t_train), (x_test, t_test)\n",
    "\n",
    "\n",
    "def get_vocab():\n",
    "    return char_to_id, id_to_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3uFQosKWCvFT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_sequence('addition.txt')\n",
    "char_to_id, id_to_char = get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "djem-PB6CvFV"
   },
   "source": [
    "## ネットワーク定義\n",
    "問5. <font color=\"Red\">以下の Seq2seq クラスを完成させてください。</font>\n",
    "\n",
    "  - Encoderクラスは、各時刻での入力を順伝播させた後、最後の時刻に対応する出力をDecoder クラスに渡します。\n",
    "  - Decoderクラスの順伝播では、まずEncoderクラスの出力を内部状態としてセットし、入力系列データを順伝播させていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TZQsu3QCvFV"
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    " \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(D, H, stateful=False)\n",
    "\n",
    "        self.params = self.embed.params + self.lstm.params\n",
    "        self.grads = self.embed.grads + self.lstm.grads\n",
    "        self.hs = None\n",
    "\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        self.hs = hs\n",
    "        return hs[######問4.1######]\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(D, H, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        self.lstm.set_state(######問4.2######)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        out = self.lstm.forward(out)\n",
    "        score = self.affine.forward(out)\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dout = self.embed.backward(dout)\n",
    "        dh = self.lstm.dh\n",
    "        return dh\n",
    "\n",
    "class Seq2seq():\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = Decoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
    "\n",
    "        h = self.encoder.forward(xs) \n",
    "        score = self.decoder.forward(decoder_xs, h) \n",
    "        loss = self.softmax.forward(score, decoder_ts) \n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.softmax.backward(dout)\n",
    "        dh = self.decoder.backward(dout)\n",
    "        dout = self.encoder.backward(dh)\n",
    "        return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnZqVX0ECvFW"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hideen_size = 128\n",
    "batch_size = 128\n",
    "max_epoch = 8\n",
    "max_grad = 5.0\n",
    "\n",
    "model = Seq2seq(vocab_size, wordvec_size, hideen_size)\n",
    "optimizer = Adam()\n",
    "\n",
    "data_size = len(x_train)\n",
    "max_iters = data_size // batch_size\n",
    "loss_list = []\n",
    "eval_interval = 50\n",
    "current_epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gkmNOSrrCvFY"
   },
   "source": [
    "### 学習、評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Shwm9X8OCvFZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    # シャッフル\n",
    "    idx = np.random.permutation(np.arange(data_size))\n",
    "    x = x_train[idx]\n",
    "    t = t_train[idx]\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "#         params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "        if max_grad is not None:\n",
    "            clip_grads(model.grads, max_grad)\n",
    "        optimizer.update(model.params,model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "        # 評価\n",
    "        if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch %d |  iter %d / %d | loss %.2f'\n",
    "                  % (current_epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(float(avg_loss))\n",
    "            total_loss, loss_count = 0, 0\n",
    "    current_epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5TtdA2_-A05",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(loss_list)), loss_list)\n",
    "plt.xlabel(\"#iter\")\n",
    "plt.ylabel(\"Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5m6Hk71s-A1D"
   },
   "source": [
    "\n",
    "## 提出可否"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dI9k0bSr-A1D"
   },
   "outputs": [],
   "source": [
    "if loss_list[-1] > 1.1:\n",
    "    print(\"Seq2seqの実装に間違いがあります。問4を見直してください。\")\n",
    "else:\n",
    "    print(\"学習成功です。提出してください\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Day5演習.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
